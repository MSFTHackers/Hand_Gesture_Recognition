{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nimport math\nimport pandas as pd\nimport matplotlib.image as img\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility functions\n# return gray image\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\n# training targets stored\ntargets_temp = pd.read_csv('/kaggle/input/gesturecsvs/jester-v1-train.csv',header=None,sep = \";\").to_dict()\ntargets = {}\nfor index in range(len(targets_temp[0])):\n  targets[targets_temp[0][index]] = targets_temp[1][index]\n\n# validation targets stored\ntargets_validation_temp = pd.read_csv('/kaggle/input/gesturecsvs/jester-v1-train.csv',header=None,sep = \";\").to_dict()\ntargets_validation = {}\n# print(list(targets_validation_temp[0].values()).index(34870))\nvalDirs = os.listdir('/kaggle/working/val_data')\nfor dir in valDirs:\n  indexDir = int(dir)\n  targets_validation[indexDir] = targets_validation_temp[1][list(targets_validation_temp[0].values()).index(indexDir)]\n    \n# Finally, classes label you want to use all labels \nlabel = pd.read_csv('/kaggle/input/gesturecsvs/jester-v1-labels.csv',header=None, usecols=[0])\nlabel.head()\ntargets_name = label[0].tolist()\nlen(targets_name)\n\n# Get the data directories\npath = \"/kaggle/input/gesturevid/20bn50_part1/\"\npath_cv = \"/kaggle/working/val_data/\"\n\ndirs = os.listdir(path)\ndirs_cv = os.listdir(path_cv)\n\n# number of samples for training and validation\nprint(len(dirs))\nprint(len(dirs_cv))\n\n# The videos do not have the same number of frames, here we try to unify.\n\nhm_frames = 30 # number of frames\n# unify number of frames for each training\ndef get_unify_frames(path):\n    offset = 0\n    # pick frames\n    frames = os.listdir(path)\n    frames_count = len(frames)\n    # unify number of frames \n    if hm_frames > frames_count:\n        # duplicate last frame if video is shorter than necessary\n        frames += [frames[-1]] * (hm_frames - frames_count)\n    elif hm_frames < frames_count:\n        # If there are more frames, then sample starting offset\n        #diff = (frames_count - hm_frames)\n        #offset = diff-1 \n        frames = frames[0:hm_frames]\n    return frames\n\n# Resize frames\ndef resize_frame(frame):\n    frame = img.imread(frame)\n    frame = cv2.resize(frame, (64, 64))\n    return frame\n\n# Function to empty the RAM\ndef release_list(a):\n   del a[:]\n   del a\n    \n#Defining loss function in use\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam()\n\n# Loss\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nvalid_loss = tf.keras.metrics.Mean(name='valid_loss')\n# Accuracy\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\nvalid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n\n@tf.function\ndef train_step(image, targets):\n    with tf.GradientTape() as tape:\n        # Make a prediction on all the batch\n        predictions = model(image)\n        # Get the error/loss on these predictions\n        loss = loss_fn(targets, predictions)\n    # Compute the gradient which respect to the loss\n    grads = tape.gradient(loss, model.trainable_variables)\n    # Change the weights of the model\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    # The metrics are accumulate over time. You don't need to average it yourself.\n    train_loss(loss)\n    train_accuracy(targets, predictions)\n    \n\n@tf.function\ndef valid_step(image, targets):\n    predictions = model(image)\n    t_loss = loss_fn(targets, predictions)\n    # Set the metrics for the test\n    valid_loss(t_loss)\n    valid_accuracy(targets, predictions)\n\n","execution_count":3,"outputs":[{"output_type":"stream","text":"59281\n500\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjusting validation data\nwith tf.device('/device:GPU:0'):\n  counter_validation = 0\n  cv_targets = []\n  new_frames_cv = []\n  for directory in dirs_cv:\n      new_frame = []\n      # Frames in each folder\n      frames = get_unify_frames(path_cv+directory)\n      if len(frames)==hm_frames:\n          for frame in frames:\n              new_frame.append(rgb2gray(cv2.imread(path_cv+directory+'/'+frame)))\n              new_frames_cv.append(new_frame)\n              if len(new_frame) == 15:\n#                   print(directory)\n                  new_frames_cv.append(new_frame)\n                  cv_targets.append(targets_name.index(targets_validation[int(directory)]))\n                  counter_validation +=1\n                  new_frame = []\n\n  # convert validation data to np float32\n  cv_data = np.array(new_frames_cv[0:counter_validation], dtype=np.float32)\n  print(cv_data.shape)\n\n  #release new frames cv\n  release_list(new_frames_cv)\n\n  # Normalisation: validation\n  print('old mean', cv_data.mean())\n  scaler = StandardScaler()\n  scaled_images_cv  = scaler.fit_transform(cv_data.reshape(-1, 15*64*64))\n  print('new mean',scaled_images_cv.mean())\n  scaled_images_cv  = scaled_images_cv.reshape(-1, 15, 64, 64, 1)\n  print(scaled_images_cv.shape)\n\n  cv_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_cv, cv_targets))\n\n","execution_count":4,"outputs":[{"output_type":"stream","text":"(1000, 15, 64, 64)\nold mean 109.43539\nnew mean 9.480318e-08\n(1000, 15, 64, 64, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convolutions\nconv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\npool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\nconv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\npool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n\n# LSTM & Flatten\nconvLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\nflatten =  tf.keras.layers.Flatten(name=\"flatten\")\n\n# Dense layers\nd1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\nout = tf.keras.layers.Dense(27, activation='softmax', name=\"output\")\n\n# Create the 3D CNN model for hand gesture recognition\nmodel = Sequential()\nmodel.add(tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last'))\nmodel.add(pool1)\nmodel.add(conv2)\nmodel.add(pool2)\nmodel.add(convLSTM)\nmodel.add(flatten)\nmodel.add(d1)\nmodel.add(out)\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\nmanager = tf.train.CheckpointManager(ckpt, '/kaggle/working/tf_ckpts', max_to_keep=10)\nckpt.restore(manager.latest_checkpoint)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff2d412c310>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Every 4000 videos\ndelta = 4000\nstart = 0\nloop = 1\nfor i in range(start, start+delta):\n  print(\"##################### LOOP STARTED\" + str(loop) + \"#########################\")\n  counter_training = 0 \n  training_targets = [] \n  new_frames = [] # training data after resize & unify\n  # Preparing training data\n  with tf.device('/device:GPU:0'):\n    # Adjust training data\n    for directory in dirs[start:start+delta]:\n      new_frame = [] \n      frames = get_unify_frames(path+directory)\n      if len(frames) == hm_frames: # just to be sure\n          for frame in frames:\n              new_frame.append(rgb2gray( tf.image.per_image_standardization(cv2.imread(path+directory+'/'+frame))))\n              new_frames.append(new_frame)\n              if len(new_frame) == 15: # partition each training on two trainings.\n                  new_frames.append(new_frame) # append each partition to training data\n                  training_targets.append(targets_name.index(targets[int(directory)]))\n                  counter_training +=1\n                  new_frame = []\n              \n    training_data = np.array(new_frames[0:counter_training], dtype=np.float32)\n    # print(training_data.shape)    \n\n    #release new frames\n    release_list(new_frames)\n\n    # Normalisation: training\n    # print('old mean', training_data.mean())\n    scaler = StandardScaler()\n    scaled_images  = scaler.fit_transform(training_data.reshape(-1, 15*64*64))\n    # print('new mean', scaled_images.mean())\n    scaled_images  = scaled_images.reshape(-1, 15, 64, 64, 1)\n    # print(\"scaled images shape:\")\n    # print(scaled_images.shape)\n\n    # release training_data array\n    del(training_data)\n\n    # use tensorflow Dataset\n    train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images, training_targets))\n\n    if (loop != 1):\n      # Load the previous saved weights\n      ckpt.restore(manager.latest_checkpoint)\n    \n    else:\n      #Necessary step to build model once with data\n      model(scaled_images[0:2])\n      model.summary()\n    \n    #release \n    del(scaled_images)\n    \n    #Training \n    epoch = 10\n    batch_size = 32\n    b = 0\n    training_acc = []\n    validation_acc = []\n    for epoch in range(epoch):\n        # Training set\n        for images_batch, targets_batch in train_dataset.batch(batch_size):\n            train_step(images_batch, targets_batch)\n            # print(\"--Train step done--\")\n            template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n            print(template.format(\n                b, len(training_targets), train_loss.result(), \n                train_accuracy.result()*100\n            ), end=\"\")\n            b += batch_size\n        # Validation set\n        for images_batch, targets_batch in cv_dataset.batch(batch_size):\n            valid_step(images_batch, targets_batch)\n\n        template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n        print(template.format(\n            epoch+1,\n            valid_loss.result(), \n            valid_accuracy.result()*100)\n        )\n        training_acc.append(float(train_accuracy.result()*100))\n        validation_acc.append(float(valid_accuracy.result()*100))\n        ckpt.step.assign_add(1)\n        # save_path = manager.save()\n        # print(\" checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n        valid_loss.reset_states()\n        valid_accuracy.reset_states()\n        train_accuracy.reset_states()\n        train_loss.reset_states()\n\n    if (loop % 2 == 0):\n      # plot Accuracy / epoch\n      plt.plot([1,2,3,4,5,6,7,8,9,10],training_acc, '-' )\n      plt.plot([1,2,3,4,5,6,7,8,9,10],validation_acc, '-' )\n\n      plt.ylabel('Accuracy')\n      plt.xlabel('Epochs')\n      plt.show()\n    # save the model for use in the application\n    save_path = manager.save()\n    \n    #release\n    del(train_dataset)\n    \n    # print(\" checkpoint for step {}: {}\".format(int(ckpt.step), save_path))    \n    start = start + delta\n    loop = loop + 1\n    if(start > 50000):\n      break","execution_count":null,"outputs":[{"output_type":"stream","text":"##################### LOOP STARTED1#########################\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1 (Conv3D)               multiple                  896       \n_________________________________________________________________\nmax_pooling3d (MaxPooling3D) multiple                  0         \n_________________________________________________________________\nconv1 (Conv3D)               multiple                  55360     \n_________________________________________________________________\nmax_pooling3d_1 (MaxPooling3 multiple                  0         \n_________________________________________________________________\nconv_lst_m2d (ConvLSTM2D)    multiple                  149920    \n_________________________________________________________________\nflatten (Flatten)            multiple                  0         \n_________________________________________________________________\nd1 (Dense)                   multiple                  737408    \n_________________________________________________________________\noutput (Dense)               multiple                  3483      \n=================================================================\nTotal params: 947,067\nTrainable params: 947,067\nNon-trainable params: 0\n_________________________________________________________________\n Batch 7968/8000, Loss: 3.2740230560302734, Accuracy: 8.1999998092651375\nEpoch 1, Valid Loss: 3.284048080444336, Valid Accuracy: 6.199999809265137\n Batch 15968/8000, Loss: 3.2734599113464355, Accuracy: 8.1999998092651375\nEpoch 2, Valid Loss: 3.2841637134552, Valid Accuracy: 6.199999809265137\n Batch 23968/8000, Loss: 3.272744655609131, Accuracy: 8.19999980926513775\nEpoch 3, Valid Loss: 3.291465997695923, Valid Accuracy: 6.199999809265137\n Batch 31968/8000, Loss: 3.2732222080230713, Accuracy: 8.1999998092651375\nEpoch 4, Valid Loss: 3.281735897064209, Valid Accuracy: 6.199999809265137\n Batch 39968/8000, Loss: 3.2729437351226807, Accuracy: 8.1999998092651375\nEpoch 5, Valid Loss: 3.2813029289245605, Valid Accuracy: 6.199999809265137\n Batch 47968/8000, Loss: 3.272780418395996, Accuracy: 8.19999980926513735\nEpoch 6, Valid Loss: 3.2838282585144043, Valid Accuracy: 6.199999809265137\n Batch 55968/8000, Loss: 3.2725961208343506, Accuracy: 8.1999998092651375\nEpoch 7, Valid Loss: 3.282299280166626, Valid Accuracy: 6.199999809265137\n Batch 63968/8000, Loss: 3.2712414264678955, Accuracy: 8.1999998092651375\nEpoch 8, Valid Loss: 3.283238172531128, Valid Accuracy: 6.199999809265137\n Batch 71968/8000, Loss: 3.272568464279175, Accuracy: 8.19999980926513735\nEpoch 9, Valid Loss: 3.281965494155884, Valid Accuracy: 6.199999809265137\n Batch 79968/8000, Loss: 3.272615909576416, Accuracy: 8.19999980926513775\nEpoch 10, Valid Loss: 3.2829418182373047, Valid Accuracy: 6.199999809265137\n##################### LOOP STARTED2#########################\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}